{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Maker\n",
    "The goal of this project is to expand upon product information supplied in regular invoice/PO data to understand what the product actually is.  This is done by mimicing human processes of googling the description and then scraping all results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from bs4.element import Comment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom functions to help streamline the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetWebResults(SearchString):\n",
    "    #Spoof the Browser\n",
    "    br = mechanize.Browser()\n",
    "    br.set_handle_robots(False)\n",
    "    br.set_handle_equiv(False)\n",
    "    br.addheaders = [('User-agent', 'Mozilla/5.0')] \n",
    "    br.open('http://www.google.com/')   \n",
    "\n",
    "    # do the query and return the text\n",
    "    br.select_form(name='f')   \n",
    "    br.form['q'] = SearchString # query\n",
    "    data = br.submit()\n",
    "    soup = BeautifulSoup(data.read(), \"html5lib\")\n",
    "\n",
    "    WebResults = []\n",
    "\n",
    "    #Put all the description words into a wordlist\n",
    "    for i in soup.findAll(\"span\", { \"class\" : \"st\" }):\n",
    "        WebResults.append(i.text)\n",
    "\n",
    "    return WebResults\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    #soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def CleanWords(listofWords):\n",
    "\n",
    "    #Assign the WebResults to a new variable so I can run this code multiple times without having to scrape the web\n",
    "    wordlist = ''.join(listofWords).lower().split()\n",
    "\n",
    "    #Remove specific Characters from the wordlist\n",
    "    wordlist = [i.replace(',',' ').replace('-',' ').replace('.',' ').replace('\\t',' ') for i in wordlist]\n",
    "\n",
    "    #Remove non-alphaNumeric and replace them with a space\n",
    "    wordlist = [re.sub('[^ a-zA-Z0-9]+', ' ', i) for i in wordlist]\n",
    "\n",
    "    #Remove double spaces and replace them with a single space\n",
    "    wordlist = [re.sub('[  ]+', ' ', i) for i in wordlist]\n",
    "    \n",
    "    #Remove the product information from the keywords (we already know it, so what add it) and lowercase the list of words\n",
    "    #wordlist = [i.lower() for i in wordlist if i.lower() not in SearchString.lower()]\n",
    "    \n",
    "    #remove specific stopwords from the list\n",
    "    stopwords = ['...', '....', ' ','', 'the', 'in', 'to','of', 'and', 'at', 'for', 'you',\n",
    "                 'with', 'is', '-', '/', 'on', 'case', 'a', 'website', 'registered', 'com',\n",
    "                 'back', 'all', 'product', 'back', 'available', 'no', 'price']\n",
    "    wordlist = [i.strip() for i in wordlist if i.strip() not in stopwords]\n",
    "\n",
    "    #Lemmitize the words to remove pluralization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    wordlist = [wordnet_lemmatizer.lemmatize(i) for i in wordlist]\n",
    "\n",
    "    print('\\nWordlist is cleaned and contains %i records\\n' %len(wordlist))\n",
    "    \n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the code to search for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords = []\n",
    "\n",
    "#Read in spreadsheet of part information\n",
    "itemList = pd.read_excel('CapitalEquipmentExamples.xlsx')\n",
    "itemList.head(2)\n",
    "\n",
    "#Format the spreadsheet for ease of use\n",
    "itemList = itemList.transpose()\n",
    "#Rename the columns\n",
    "itemList.columns = itemList.iloc[0]\n",
    "#Drop the old header in favor of the indexed header\n",
    "itemList = itemList.drop(itemList.index[0])\n",
    "\n",
    "#Create the search term column\n",
    "itemList['SearchTerm'] = itemList['Manufacturer Part Number'].astype(str) + ' ' + itemList['Manufacturer Name']\n",
    "\n",
    "itemList = itemList.loc[['B001170213'], :]\n",
    "\n",
    "for i in itemList['SearchTerm']:\n",
    "    #Run the code to get webresults\n",
    "    WebResults = GetWebResults(i)\n",
    "    wordlist = CleanWords(WebResults)\n",
    "    \n",
    "    #Count frequency on the wordlist\n",
    "    counts = collections.Counter(wordlist)\n",
    "    new_list = pd.DataFrame(counts.most_common(), columns=['Word', 'Frequency'])\n",
    "    \n",
    "    #Append to the new_list the information from the source data\n",
    "    \n",
    "    \n",
    "    print(i)\n",
    "    print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct frequency counts on words\n",
    "Count each and every word and then sort by word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate the counts of keywords in the descriptions\n",
    "counts = collections.Counter(wordlist)\n",
    "new_list = pd.DataFrame(counts.most_common(), columns=['Word', 'Frequency'])\n",
    "\n",
    "new_list[new_list['Frequency'] >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the data for AI\n",
    "Create a dataframe that contains the string of words and the SKU/Manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spoof the Browser\n",
    "br = mechanize.Browser()\n",
    "br.set_handle_robots(False)\n",
    "br.set_handle_equiv(False)\n",
    "br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
    "br.open('http://www.google.com/')   \n",
    "\n",
    "# do the query and return the text\n",
    "br.select_form(name='f')   \n",
    "br.form['q'] = 'Leica Microsystems 10450294 Leica Microsystems 10450294'\n",
    "data = br.submit()\n",
    "soup = BeautifulSoup(data.read(), \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    #soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def CleanWords(listofWords):\n",
    "\n",
    "    #Assign the WebResults to a new variable so I can run this code multiple times without having to scrape the web\n",
    "    wordlist = ''.join(listofWords).lower().split()\n",
    "\n",
    "    #Remove specific Characters from the wordlist\n",
    "    wordlist = [i.replace(',',' ').replace('-',' ').replace('.',' ').replace('\\t',' ') for i in wordlist]\n",
    "\n",
    "    #Remove non-alphaNumeric and replace them with a space\n",
    "    wordlist = [re.sub('[^ a-zA-Z0-9]+', ' ', i) for i in wordlist]\n",
    "\n",
    "    #Remove double spaces and replace them with a single space\n",
    "    wordlist = [re.sub('[  ]+', ' ', i) for i in wordlist]\n",
    "    \n",
    "    #Remove the product information from the keywords (we already know it, so what add it) and lowercase the list of words\n",
    "    #wordlist = [i.lower() for i in wordlist if i.lower() not in SearchString.lower()]\n",
    "    \n",
    "    #remove specific stopwords from the list\n",
    "    stopwords = ['...', '....', ' ','', 'the', 'in', 'to','of', 'and', 'at', 'for', 'you',\n",
    "                 'with', 'is', '-', '/', 'on', 'case', 'a', 'website', 'registered', 'com',\n",
    "                 'back', 'all', 'product', 'back', 'available', 'no', 'price']\n",
    "    wordlist = [i.strip() for i in wordlist if i.strip() not in stopwords]\n",
    "\n",
    "    #Lemmitize the words to remove pluralization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    wordlist = [wordnet_lemmatizer.lemmatize(i) for i in wordlist]\n",
    "\n",
    "    print('\\nWordlist is cleaned and contains %i records\\n' %len(wordlist))\n",
    "    \n",
    "    return wordlist\n",
    "\n",
    "myWords = []\n",
    "myLink = []\n",
    "keywords = []\n",
    "#Find the text of the first link and get the text\n",
    "for i in soup.find_all(\"h3\", {\"class\" : \"r\"}):\n",
    "    myLink.append(i.get_text())\n",
    "    \n",
    "for i in myLink[0:5]:\n",
    "    print i\n",
    "    if not '.pdf' in str(br.find_link(text=i)):\n",
    "        #r.find_link(text=i)\n",
    "        req = br.click_link(text=i)\n",
    "        soup = BeautifulSoup(br.open(req), \"html5lib\") \n",
    "        try:\n",
    "            keywords.append(soup.find(\"meta\", {\"name\" : \"keywords\"})['content'])\n",
    "            myWords.append(text_from_html(soup))\n",
    "            print('%s found keywords' % br.geturl())\n",
    "        except:\n",
    "            print('%s could not find keywords' % br.geturl())          \n",
    "\n",
    "        \n",
    "        br.back()\n",
    "\n",
    "#for div in soup.findAll(\"h3\", {\"class\" : \"r\"}):\n",
    "#    for a in div.findAll('a'):\n",
    "#        print a.get('href')[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = CleanWords(myWords)\n",
    "counts = collections.Counter(A)\n",
    "pd.DataFrame(counts.most_common(), columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"meta\", {\"name\" : \"^keywords\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.lower() for i in str(''.join(soup))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
